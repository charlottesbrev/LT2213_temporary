{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation using Neural Networks\n",
    "Adam Ek\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Before starting, please read the instructions on [how to work on group assignments](https://github.com/sdobnik/computational-semantics/blob/master/README.md).\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with static distributional vectors is the difficulty of distinguishing between different *word senses*. We will continue our exploration of word vectors by considering *trainable vectors* or *word embeddings* for Word Sense Disambiguation (WSD).\n",
    "\n",
    "The goal of word sense disambiguation is to train a model to find the sense of a word (homonyms of a word-form). For example, the word \"bank\" can mean \"sloping land\" or \"financial institution\". \n",
    "\n",
    "(a) \"I deposited my money in the **bank**\" (financial institution)\n",
    "\n",
    "(b) \"I swam from the river **bank**\" (sloping land)\n",
    "\n",
    "In case a) and b) we can determine that the meaning of \"bank\" based on the *context*. To utilize context in a semantic model we use *contextualized word representations*. Previously we worked with *static word representations*, i.e. the representation does not depend on the context. To illustrate we can consider sentences (a) and (b), the word **bank** would have the same static representation in both sentences, which means that it becomes difficult for us to predict its sense. What we want is to create representations that depend on the context, i.e. *contextualized embeddings*. \n",
    "\n",
    "We will create contextualized embeddings with Recurrent Neural Networks. You can read more about recurrent neural netoworks [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). Your overall task in this lab is to create a neural network model that can disambiguate the word sense of 15 different words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we import some packages that we need\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "\n",
    "# our hyperparameters (add more when/if you need them)\n",
    "device = torch.device('cuda:0') # Use the MLT GPU when you can!\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documenting you code\n",
    "**Note:** This lab is focused quite abit on programming and working with neural networs, i.e. writing code. Comment the code that you write and explain what it does, the code documentation will be taken into account when grading. Also it's very beneficial for you to explain to yourself what the code is doing, and it helps me give feedback to you :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Working with data\n",
    "\n",
    "A central part of any machine learning system is the data we're working with. In this section we will split the data (the dataset is located here: ``wsd-data/wsd_data.txt``) into a training set and a test set. We will also create a baseline to compare our model against. Finally, we will use TorchText to transform our data (raw text) into a convenient format that our neural network can work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The dataset we will use contain different word sense for 15 different words. The data is organized as follows (values separated by tabs): \n",
    "- Column 1: word-sense\n",
    "- Column 2: word-form\n",
    "- Column 3: index of word\n",
    "- Column 4: white-space tokenized context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data\n",
    "\n",
    "Your first task is to seperate the data into a *training set* and a *test set*. The training set should contain 80% of the examples and the test set the remaining 20%. The examples for the test/training set should be selected **randomly**. Save each dataset into a .csv file for loading later. **[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def data_split(path_to_dataset):\n",
    "    # content will hold all data from the file\n",
    "    content = []\n",
    "    # read the content\n",
    "    with open(path_to_dataset, encoding='utf-8') as f:\n",
    "        content = f.readlines()\n",
    "    # calculate how many lines should be training / testing\n",
    "    number_of_lines = len(content)\n",
    "    number_of_training_lines = int(number_of_lines * 0.8)\n",
    "    number_of_test_lines = number_of_lines - number_of_training_lines\n",
    "\n",
    "    # create list of all indices\n",
    "    all_indices = list(range(0, number_of_lines))\n",
    "    # randomly select data for testing\n",
    "    random_indices = random.sample(all_indices, number_of_test_lines)\n",
    "    #print(all_indices)\n",
    "    #print(random_indices)\n",
    "    \n",
    "    # split the data to a testing list and a training list\n",
    "    testing_list = []\n",
    "    training_list = []\n",
    "    for i in range(0, number_of_lines):\n",
    "        if i in random_indices:\n",
    "            testing_list.append(content[i])\n",
    "        else:\n",
    "            training_list.append(content[i])\n",
    "    # return the lists\n",
    "    return training_list, testing_list\n",
    "\n",
    "# function to save the contents to a csv-file\n",
    "def save_to_csv(data, filename):\n",
    "    text_file = open(filename, \"w\", encoding=\"utf-8\")\n",
    "    for line in data:\n",
    "        text_file.write(line)\n",
    "    text_file.close()\n",
    "\n",
    "# read the input and split to train and test sets\n",
    "train,test = data_split(\"wsd-data/wsd_data.txt\")\n",
    "# save the train set\n",
    "save_to_csv(train, \"wsd-data/train.csv\")\n",
    "# save the test set\n",
    "save_to_csv(test, \"wsd-data/test.csv\")\n",
    "\n",
    "#print(train)\n",
    "#print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a baseline\n",
    "\n",
    "Your second task is to create a *baseline* for the task. A baseline is a \"reality check\" for a model, given a very simple heuristic/algorithmic/model solution to the problem, can our neural network perform better than this?\n",
    "The baseline you are to create is the \"most common sense\" (MCS) baseline. For each word form, find the most commonly assigned sense to the word, and label a words with that sense. **[2 marks]**\n",
    "\n",
    "E.g. In a fictional dataset, \"bank\" have two senses, \"financial institution\" which occur 5 times and \"side of river\" 3 times. Thus, all 8 occurences of bank is labeled \"financial institution\" and this yields an MCS accuracy of 5/8 = 62.5%. If a model obtain a higher score than this, we can conclude that the model *at least* is better than selecting the most frequent word sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dictionary:\n",
    "# use as follows: dict[word][form] : [most common sense, freq (%)]\n",
    "def mcs_baseline(data):\n",
    "    # word: pick word from column 0, the characters until '%'\n",
    "    # sense: pick sense from column 0, the 4'th ':' string is the word sense\n",
    "    # form: pick form from column 1, split on '.' pick the second in list (common.a) => a\n",
    "\n",
    "    # temporary dictionary d\n",
    "    # used as: count = d[word][form][sense]\n",
    "    d = {}\n",
    "    for line in data:\n",
    "        # example text: line=\"keep%2:42:07:cache:\tkeep.v\t15\tAction by ...\"\n",
    "        columns = line.split('\\t') # split by tabs\n",
    "        # columns[0] = \"keep%2:42:07:cache:\"\n",
    "        # columns[1] = \"keep.v\"\n",
    "        # columns[2] = \"15\"\n",
    "        # columns[3] = \"Action by ...\"\n",
    "        #print(columns[0] + \" \" + columns[1])\n",
    "        cs = columns[0].split('%')\n",
    "        word = cs[0] # pick the first element (before '%') as word\n",
    "        form = columns[1].split('.')[1] # pick second element after '.' as form\n",
    "        # cs[1] example: \"2:42:07:cache:\"\n",
    "        split_for_sense = cs[1].split(':')\n",
    "        sense = split_for_sense[3]\n",
    "        # now we have word, form, sense\n",
    "        # increment d[word][form][sense]\n",
    "        if not word in d:\n",
    "            d[word] = {}\n",
    "        if not form in d[word]:\n",
    "            d[word][form] = {}\n",
    "        if not sense in d[word][form]:\n",
    "            d[word][form][sense] = 1 # if we have no previous counter, set it to 1\n",
    "        else:\n",
    "            d[word][form][sense] += 1 # increment\n",
    "        #print('word: ' + word)\n",
    "        #print('form: ' + form)\n",
    "        #print('sense: ' + sense)\n",
    "\n",
    "    # create the dictionary we want to return\n",
    "    # e[word][form] = [most common sense, most common frequency %]\n",
    "    e = {}\n",
    "    # used as: count = d[word][form][sense]\n",
    "    for word, form_sense_dict in d.items():\n",
    "        e[word] = {}\n",
    "        for form, sense_statistics in form_sense_dict.items():\n",
    "            #print(sense_statistics)\n",
    "            sum = 0\n",
    "            mc_freq = 0\n",
    "            mc_sense = ''\n",
    "            # find the most common sense and most common frequency\n",
    "            for sense, counter in sense_statistics.items():\n",
    "                if mc_freq < counter:\n",
    "                    mc_freq = counter\n",
    "                    mc_sense = sense\n",
    "                sum += counter\n",
    "            # evaluate frequency as %\n",
    "            mc_freq = mc_freq / sum\n",
    "            #print(\"most common sense: '\" + mc_sense + \"': \" + str(mc_freq))\n",
    "            e[word][form] = [mc_sense, mc_freq]\n",
    "    return e\n",
    "\n",
    "# create a baseline from the training set\n",
    "mcs_dict = mcs_baseline(train)\n",
    "#print(mcs_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating data iterators\n",
    "\n",
    "To train a neural network, we first need to prepare the data. This involves converting words (and labels) to a number, and organizing the data into batches. We also want the ability to shuffle the examples such that they appear in a random order.  \n",
    "\n",
    "To do all of this we will use the torchtext library (https://torchtext.readthedocs.io/en/latest/index.html). In addition to converting our data into numerical form and creating batches, it will generate a word and label vocabulary, and data iterators than can sort and shuffle the examples. \n",
    "\n",
    "Your task is to create a dataloader for the training and test set you created previously. So, how do we go about doing this?\n",
    "\n",
    "1) First we create a ``Field`` for each of our columns. A field is a function which tokenize the input, keep a dictionary of word-to-numbers, and fix paddings. So, we need four fields, one for the word-sense, one for the position, one for the lemma and one for the context. \n",
    "\n",
    "2) After we have our fields, we need to process the data. For this we use the ``TabularDataset`` class. We pass the name and path of the training and test files we created previously, then we assign which field to use in each column. The result is that each column will be processed by the field indicated. So, the context column will be tokenized and processed by the context field and so on. \n",
    "\n",
    "3) After we have processed the dataset we need to build the vocabulary, for this we call the function ``build_vocab()`` on the different ``Fields`` with the output from ``TabularDataset`` as input. This looks at our dataset and creates the necessary vocabularies (word-to-number mappings). \n",
    "\n",
    "4) Finally, the last step. In the last step we load the data objects given by the ``TabularDataset`` and pass it to the ``BucketIterator`` class. This class will organize our examples into batches and shuffle them around (such that for each epoch the model observe the examples in a different order). When we are done with this we can let our function return the data iterators and vocabularies, then we are ready to train and test our model!\n",
    "\n",
    "Implement the dataloader. [**4 marks**]\n",
    "\n",
    "*hint: for TabularDataset and BucketIterator use the class function splits()* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(path):\n",
    "    # your code goes here\n",
    "    ...\n",
    "\n",
    "dataloader(\"wsd-data/test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating and running a Neural Network for WSD\n",
    "\n",
    "In this section we will create and run a neural network to predict word senses based on *contextualized representations*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We will use a bidirectional Long-Short-Term Memory (LSTM) network to create a representation for the sentences and a Linear classifier to predict the sense of each word.\n",
    "\n",
    "When we initialize the model, we need a few things:\n",
    "\n",
    "    1) An embedding layer: a dictionary from which we can obtain word embeddings\n",
    "    2) A LSTM-module to obtain contextual representations\n",
    "    3) A classifier that compute scores for each word-sense given *some* input\n",
    "\n",
    "\n",
    "The general procedure is the following:\n",
    "\n",
    "    1) For each word in the sentence, obtain word embeddings\n",
    "    2) Run the embedded sentences through the RNN\n",
    "    3) Select the appropriate hidden state\n",
    "    4) Predict the word-sense \n",
    "\n",
    "**Suggestion for efficiency:**  *Use a low dimensionality (32) for word embeddings and the LSTM when developing and testing the code, then scale up when running the full training/tests*\n",
    "    \n",
    "Your tasks will be to create two different models (both follow the two outlines described above), described below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first approach to WSD, you are to select the index of our target word (column 3 in the dataset) and predict the word sense. **[8 marks]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSDModel_approach1(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        # your code goes here\n",
    "        self.embeddings = ...\n",
    "        self.rnn = ...\n",
    "        self.classifier = ...\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        # your code goes here\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second approach to WSD, you are to predict the word sense based on the final hidden state given by the RNN. **[8 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSDModel_approach2(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        # your code goes here\n",
    "        self.embeddings = ...\n",
    "        self.rnn = ...\n",
    "        self.classifier = ...\n",
    "    \n",
    "    def forward(self, ...):\n",
    "        # your code goes here\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing the model\n",
    "\n",
    "Now we are ready to train and test our model. What we need now is a loss function, an optimizer, and our data. \n",
    "\n",
    "- First, create the loss function and the optimizer.\n",
    "- Next, we iterate over the number of epochs (i.e. how many times we let the model see our data). \n",
    "- For each epoch, iterate over the dataset (``train_iter``) to obtain batches. Use the batch as input to the model, and let the model output scores for the different word senses.\n",
    "- For each model output, calculate the loss (and print the loss) on the output and update the model parameters.\n",
    "- Reset the gradients and repeat.\n",
    "- After all epochs are done, test your trained model on the test set (``test_iter``) and calculate the total and per-word-form accuracy of your model.\n",
    "\n",
    "Implement the training and testing of the model **[4 marks]**\n",
    "\n",
    "**Suggestion for efficiency:** *when developing your model, try training and testing the model on one or two batches (for each epoch) of data to make sure everything works! It's very annoying if you train for N epochs to find out that something went wrong when testing the model, or to find that something goes wrong when moving from epoch 0 to epoch 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = dataloader(path_to_folder)\n",
    "\n",
    "loss_function = ...\n",
    "optimizer = ...\n",
    "model = ...\n",
    "\n",
    "for _ in range(epochs):\n",
    "    # train model\n",
    "    ...\n",
    "    \n",
    "# test model after all epochs are completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the difference between the first and second approach. What kind of representations are the different approaches using to predict word-senses? **[5 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model with per-word-form *accuracy* and comment on the results you get, how does the model perform in comparison to the baseline, and how does the first approach compare to the second? Which one is more successful? **[5 marks]**\n",
    "\n",
    "**Your answer should go here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could we do to improve our word sense disambiguation model? **[8 marks]**\n",
    "\n",
    "**Your answer should go here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings:\n",
    "\n",
    "[1] Kågebäck, M., & Salomonsson, H. (2016). Word Sense Disambiguation using a Bidirectional LSTM. arXiv preprint arXiv:1606.03568.\n",
    "\n",
    "[2] https://cl.lingfil.uu.se/~nivre/master/NLP-LexSem.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
